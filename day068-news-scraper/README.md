# ğŸŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ã‚¢ãƒ—ãƒª (News Scraper)

Hacker Newsã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã™ã‚‹Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚åŸºæœ¬ç‰ˆã¨CSVä¿å­˜å¯¾å¿œã®æ‹¡å¼µç‰ˆã®2ç¨®é¡ã‚’å®Ÿè£…ã—ã€Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®åŸºç¤ã‹ã‚‰å®Ÿç”¨çš„ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¾ã§å­¦ç¿’ã§ãã¾ã™ã€‚

## ğŸ“ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ¦‚è¦

**ä¸»ãªæ©Ÿèƒ½**:
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—**: Hacker Newsã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å³åº§ã«å–å¾—
- **HTMLã®è‡ªå‹•è§£æ**: BeautifulSoupã«ã‚ˆã‚‹é«˜ç²¾åº¦ãªã‚¿ã‚°è§£æ
- **URLè£œå®Œæ©Ÿèƒ½**: å†…éƒ¨ãƒªãƒ³ã‚¯ã®è‡ªå‹•è£œå®Œã§å®Œå…¨ãªURLè¡¨ç¤º
- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: é€šä¿¡ã‚¨ãƒ©ãƒ¼ã‚„HTMLæ§‹é€ å¤‰æ›´ã¸ã®å¯¾å¿œ
- **CSVä¿å­˜æ©Ÿèƒ½**: ãƒ‡ãƒ¼ã‚¿ã®æ°¸ç¶šåŒ–ã¨å¾Œå‡¦ç†å¯¾å¿œï¼ˆæ‹¡å¼µç‰ˆã®ã¿ï¼‰

**å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ**:
- Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®åŸºæœ¬æŠ€è¡“
- HTMLã®æ§‹é€ ç†è§£ã¨ãƒ‡ãƒãƒƒã‚°æ‰‹æ³•
- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®é©åˆ‡ãªé¸æŠã¨çµ„ã¿åˆã‚ã›
- æ®µéšçš„ãªæ©Ÿèƒ½å®Ÿè£…ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
day68-news-scraper/
â”œâ”€â”€ main2.py          # ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼ˆåŸºæœ¬æ©Ÿèƒ½ã®ã¿ï¼‰
â”œâ”€â”€ main.py           # æ‹¡å¼µç‰ˆï¼ˆCSVä¿å­˜ãƒ»çµ±è¨ˆæ©Ÿèƒ½ä»˜ãï¼‰
â”œâ”€â”€ debug.py          # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼ˆHTMLæ§‹é€ èª¿æŸ»ï¼‰
â””â”€â”€ README.md         # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

### ğŸ¯ **å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¹å‰²**

#### **main2.pyï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰**
- **ç›®çš„**: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã®å³åº§ç¢ºèª
- **ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**: `requests`, `beautifulsoup4`
- **æ©Ÿèƒ½**: åŸºæœ¬çš„ãªå–å¾—ãƒ»è¡¨ç¤ºã®ã¿
- **é©ç”¨å ´é¢**: å­¦ç¿’ç”¨ã€ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã€è»½é‡å®Ÿè¡Œ

#### **main.pyï¼ˆæ‹¡å¼µç‰ˆï¼‰**
- **ç›®çš„**: åŒ…æ‹¬çš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ç®¡ç†
- **ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**: `requests`, `beautifulsoup4`, `csv`, `datetime`
- **æ©Ÿèƒ½**: CSVä¿å­˜ã€çµ±è¨ˆåˆ†æã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¨˜éŒ²
- **é©ç”¨å ´é¢**: ãƒ‡ãƒ¼ã‚¿åˆ†æã€å®šæœŸå®Ÿè¡Œã€å±¥æ­´ç®¡ç†

#### **debug.pyï¼ˆãƒ‡ãƒãƒƒã‚°ç‰ˆï¼‰**
- **ç›®çš„**: HTMLæ§‹é€ ã®è©³ç´°èª¿æŸ»
- **æ©Ÿèƒ½**: ã‚¿ã‚°æ•°èª¿æŸ»ã€ã‚¯ãƒ©ã‚¹è¦ç´ æ¤œå‡ºã€HTMLæŠœç²‹è¡¨ç¤º
- **é©ç”¨å ´é¢**: å•é¡Œç™ºç”Ÿæ™‚ã®åŸå› èª¿æŸ»

## ğŸš€ å®Ÿè¡Œæ–¹æ³•

### ğŸ“‹ **å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**

```bash
pip install requests beautifulsoup4
```

**æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**ï¼ˆè¿½åŠ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼‰:
- `csv`: CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ç”¨
- `datetime`: æ™‚åˆ»å‡¦ç†ç”¨

### ğŸ’» **å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰**

#### **åŸºæœ¬ç‰ˆï¼ˆè»½é‡ãƒ»é«˜é€Ÿï¼‰**
```bash
cd day68-news-scraper
python main2.py
```

#### **æ‹¡å¼µç‰ˆï¼ˆå¤šæ©Ÿèƒ½ï¼‰**
```bash
python main.py
```

#### **ãƒ‡ãƒãƒƒã‚°ç‰ˆï¼ˆå•é¡Œèª¿æŸ»ç”¨ï¼‰**
```bash
python debug.py
```

## ğŸ’¡ ä½¿ã„æ–¹

### ğŸ® **åŸºæœ¬ç‰ˆï¼ˆmain2.pyï¼‰ã®å®Ÿè¡Œä¾‹**

```bash
python main2.py
```

**å‡ºåŠ›ä¾‹**:
```
ğŸŒ Hacker Newsã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­: https://news.ycombinator.com/
ğŸ“° Hacker News ã‚¿ã‚¤ãƒˆãƒ«ä¸€è¦§ (31ä»¶):
 1. Hidden interface controls are affecting usability
    ğŸ”— https://interactions.acm.org/archive/view/july-august-2025/...
 2. July 5, 1687: When Newton Explained Why You Don't Float Away
    ğŸ”— https://multiverseemployeehandbook.com/blog/...
 3. Serving 200M requests per day with a CGI-bin
    ğŸ”— https://simonwillison.net/2025/Jul/5/cgi-bin-performance/
...ï¼ˆæœ€å¤§20ä»¶ã¾ã§è¡¨ç¤ºï¼‰
```

### ğŸ¯ **æ‹¡å¼µç‰ˆï¼ˆmain.pyï¼‰ã®å®Ÿè¡Œä¾‹**

```bash
python main.py
```

**å‡ºåŠ›ä¾‹**:
```
ğŸŒ Hacker News æ‹¡å¼µç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼é–‹å§‹
ğŸ“° å–å¾—æˆåŠŸ: 31ä»¶
============================================================
 1. Hidden interface controls are affecting usability
    ğŸ”— https://interactions.acm.org/archive/view/july-august-2025/...
    ğŸ“Š æ–‡å­—æ•°: 45 | ã‚¿ã‚¤ãƒ—: external

 2. July 5, 1687: When Newton Explained Why You Don't Float Away
    ğŸ”— https://multiverseemployeehandbook.com/blog/...
    ğŸ“Š æ–‡å­—æ•°: 54 | ã‚¿ã‚¤ãƒ—: external
...

ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ hackernews_20250706_143025.csv ã«ä¿å­˜ã—ã¾ã—ãŸ

ğŸ“Š çµ±è¨ˆæƒ…å ±:
  â€¢ ç·ä»¶æ•°: 31ä»¶
  â€¢ å¤–éƒ¨ãƒªãƒ³ã‚¯: 28ä»¶
  â€¢ å†…éƒ¨ãƒªãƒ³ã‚¯: 3ä»¶
  â€¢ å¹³å‡ã‚¿ã‚¤ãƒˆãƒ«é•·: 42.3æ–‡å­—
  â€¢ æœ€é•·ã‚¿ã‚¤ãƒˆãƒ«: Game publishers respond to Stop Killing Games... (89æ–‡å­—)
  â€¢ æœ€çŸ­ã‚¿ã‚¤ãƒˆãƒ«: X-Clacks-Overhead (17æ–‡å­—)
```

### ğŸ” **ç”Ÿæˆã•ã‚Œã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹é€ **

```csv
title,url,type,length,timestamp
"Hidden interface controls are affecting usability","https://interactions.acm.org/...",external,45,"2025-07-06T14:30:25.123456"
"July 5, 1687: When Newton Explained Why You Don't Float Away","https://multiverseemployeehandbook.com/...",external,54,"2025-07-06T14:30:25.123457"
```

## ğŸ“– å­¦ã‚“ã ã“ã¨ã‚„ä»Šå¾Œã®æ”¹å–„æ¡ˆï¼ˆå­¦ç¿’ãƒ­ã‚°ï¼‰

### ğŸ”§ ä¸»è¦ãªå­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ

#### 1ï¸âƒ£ **ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å½¹å‰²åˆ†æ‹…ã¨é€£æº**

**å®Œç’§ãª4ã¤ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªé€£æºã‚’å®Ÿæ„Ÿ**:

##### ğŸŒ **requests** - HTTPé€šä¿¡ã®é­”æ³•ä½¿ã„
```python
response = requests.get("https://news.ycombinator.com/", headers=headers, timeout=10)
response.raise_for_status()
```

**å½¹å‰²**:
- **Webãƒšãƒ¼ã‚¸ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹**: HTTP GETãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
- **User-Agentè¨­å®š**: ãƒœãƒƒãƒˆå¯¾ç­–å›é¿ã®ãŸã‚ã®ãƒ–ãƒ©ã‚¦ã‚¶å½è£…
- **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆåˆ¶å¾¡**: 10ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
- **ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯**: HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ç•°å¸¸æ¤œå‡º

**å­¦ç¿’æˆæœ**: 
- ã‚‚ã—requestsãŒç„¡ã‘ã‚Œã°æ•°ç™¾è¡Œã®ä½ãƒ¬ãƒ™ãƒ«ã‚½ã‚±ãƒƒãƒˆé€šä¿¡å®Ÿè£…ãŒå¿…è¦
- HTTPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®è©³ç´°ã‚’æ„è­˜ã›ãšã«é«˜åº¦ãªé€šä¿¡ãŒå¯èƒ½

##### ğŸ·ï¸ **BeautifulSoup** - HTMLã‚¿ã‚°æ•´ç†ã®è·äºº
```python
soup = BeautifulSoup(response.text, "html.parser")
title_elements = soup.find_all(class_="title")
link = title_elem.find("a")
title = link.get_text(strip=True)
href = link.get('href', '')
```

**å½¹å‰²**:
- **HTMLè§£æ**: æ–‡å­—åˆ—HTMLã‚’æ“ä½œå¯èƒ½ãªæ§‹é€ ã«å¤‰æ›
- **è¦ç´ æ¤œç´¢**: ã‚¯ãƒ©ã‚¹åã‚„ã‚¿ã‚°åã«ã‚ˆã‚‹æŸ”è»Ÿãªæ¤œç´¢
- **ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º**: ã‚¿ã‚°å†…ã®ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã¨å‰å¾Œç©ºç™½é™¤å»
- **å±æ€§å–å¾—**: hrefå±æ€§ãªã©ã®å–å¾—

**å­¦ç¿’æˆæœ**:
- æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹ç„¡ç†ã‚„ã‚ŠãªHTMLè§£æã¨æ¯”ã¹ã¦åœ§å€’çš„ã«å®‰å…¨ãƒ»ç¢ºå®Ÿ
- HTMLæ§‹é€ ã®å¤‰åŒ–ã«ã‚‚æŸ”è»Ÿã«å¯¾å¿œå¯èƒ½

##### ğŸ“Š **csv** - ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã®ç§˜æ›¸
```python
filename = f"hackernews_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
writer = csv.DictWriter(f, fieldnames=['title', 'url', 'type', 'length', 'timestamp'])
writer.writeheader()
writer.writerows(news_data)
```

**å½¹å‰²**:
- **æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ä¿å­˜**: è¾æ›¸å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•çš„ã«CSVå½¢å¼ã«å¤‰æ›
- **ãƒ˜ãƒƒãƒ€ãƒ¼ç®¡ç†**: åˆ—åã®è‡ªå‹•è¨­å®š
- **ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†**: ã‚«ãƒ³ãƒã‚„æ”¹è¡Œã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã®å®‰å…¨ãªä¿å­˜
- **æ–‡å­—åŒ–ã‘é˜²æ­¢**: UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹æ—¥æœ¬èªå¯¾å¿œ

##### ğŸ“… **datetime** - æ™‚é–“ç®¡ç†ã®ã‚¹ãƒšã‚·ãƒ£ãƒªã‚¹ãƒˆ
```python
'timestamp': datetime.now().isoformat()
filename = f"hackernews_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
```

**å½¹å‰²**:
- **ç¾åœ¨æ™‚åˆ»å–å¾—**: é«˜ç²¾åº¦ãªç¾åœ¨æ™‚åˆ»ã®å–å¾—
- **ISOå½¢å¼å¤‰æ›**: æ¨™æº–çš„ãªæ—¥æ™‚æ–‡å­—åˆ—ã¸ã®å¤‰æ›
- **ãƒ•ã‚¡ã‚¤ãƒ«åç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: å¹´æœˆæ—¥æ™‚åˆ†ç§’å½¢å¼ã§ã®æ–‡å­—åˆ—ç”Ÿæˆ
- **ä¸€æ„æ€§ç¢ºä¿**: åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«åã®é‡è¤‡é˜²æ­¢

#### 2ï¸âƒ£ **Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®æœ¬è³ªçš„ç†è§£**

##### ğŸ¯ **å•é¡Œç™ºç”Ÿâ†’ãƒ‡ãƒãƒƒã‚°â†’è§£æ±ºã®ãƒ—ãƒ­ã‚»ã‚¹**

**å®Ÿéš›ã«ä½“é¨“ã—ãŸæµã‚Œ**:
```
1. åˆæœŸå®Ÿè£…: class="titlelink"ã§ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ã‚’è©¦è¡Œ
   â†“
2. å•é¡Œç™ºç”Ÿ: 0ä»¶å–å¾—ï¼ˆå–å¾—å¤±æ•—ï¼‰
   â†“
3. ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œ: debug.pyã§å®Ÿéš›ã®HTMLæ§‹é€ èª¿æŸ»
   â†“
4. åŸå› ç™ºè¦‹: class="title"ãŒæ­£ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼ˆ61å€‹ç™ºè¦‹ï¼‰
   â†“
5. ä¿®æ­£å®Ÿè£…: æ­£ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã«å¤‰æ›´
   â†“
6. æˆåŠŸ: 31ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—æˆåŠŸ
```

**é‡è¦ãªå­¦ç¿’**:
- **ä»®å®šã‚ˆã‚Šèª¿æŸ»**: æƒ³åƒã§ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãã‚ˆã‚Šå®Ÿéš›ã®HTMLç¢ºèªãŒé‡è¦
- **æ®µéšçš„ãƒ‡ãƒãƒƒã‚°**: å•é¡Œã‚’ç´°åˆ†åŒ–ã—ã¦ä¸€ã¤ãšã¤æ¤œè¨¼
- **æŸ”è»Ÿãªå¯¾å¿œ**: ã‚µã‚¤ãƒˆæ§‹é€ ã®å¤‰æ›´ã«å¯¾ã™ã‚‹é©å¿œèƒ½åŠ›

##### ğŸ” **HTMLæ§‹é€ ç†è§£ã®é‡è¦æ€§**

**æƒ³å®šã—ã¦ã„ãŸæ§‹é€  vs å®Ÿéš›ã®æ§‹é€ **:
```html
<!-- æƒ³å®šï¼ˆé–“é•ã„ï¼‰ -->
<a class="titlelink">ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«</a>

<!-- å®Ÿéš›ï¼ˆæ­£è§£ï¼‰ -->
<td class="title">
    <a href="...">ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«</a>
</td>
```

**å­¦ç¿’æˆæœ**:
- **2æ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: è¦ªè¦ç´ â†’å­è¦ç´ ã®é †ã§ç¢ºå®Ÿã«å–å¾—
- **ãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ**: å…ˆã«HTMLæ§‹é€ ã‚’èª¿æŸ»ã—ã¦ã‹ã‚‰ã‚³ãƒ¼ãƒ‰å®Ÿè£…
- **è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œ**: è¤‡æ•°ã®å–å¾—æ–¹æ³•ã‚’ç”¨æ„ã—ã¦å …ç‰¢æ€§å‘ä¸Š

##### ğŸ›¡ï¸ **User-Agentã«ã‚ˆã‚‹ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡å›é¿**

```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}
response = requests.get(url, headers=headers)
```

**å­¦ç¿’å†…å®¹**:
- **ãƒœãƒƒãƒˆå¯¾ç­–ã®å­˜åœ¨**: å¤šãã®ã‚µã‚¤ãƒˆãŒè‡ªå‹•ã‚¢ã‚¯ã‚»ã‚¹ã‚’åˆ¶é™
- **ãƒ–ãƒ©ã‚¦ã‚¶å½è£…ã®å¿…è¦æ€§**: æ­£å¸¸ãªãƒ–ãƒ©ã‚¦ã‚¶ã‚¢ã‚¯ã‚»ã‚¹ã®ã‚ˆã†ã«è¦‹ã›ã‚‹æŠ€è¡“
- **å€«ç†çš„é…æ…®**: éåº¦ãªã‚¢ã‚¯ã‚»ã‚¹ã‚’é¿ã‘ã‚‹è²¬ä»»ã‚ã‚‹åˆ©ç”¨

#### 3ï¸âƒ£ **æ®µéšçš„æ©Ÿèƒ½å®Ÿè£…ã®ä¾¡å€¤**

##### ğŸ“ˆ **æ©Ÿèƒ½æ‹¡å¼µã®æµã‚Œ**

```python
# Stage 1: æœ€å°æ§‹æˆï¼ˆmain2.pyï¼‰
import requests
from bs4 import BeautifulSoup
# ç›®çš„: åŸºæœ¬æ©Ÿèƒ½ã®ç¢ºå®Ÿãªå®Ÿè£…

# Stage 2: ä¿å­˜æ©Ÿèƒ½è¿½åŠ 
import csv
# ç›®çš„: ãƒ‡ãƒ¼ã‚¿ã®æ°¸ç¶šåŒ–

# Stage 3: æ™‚åˆ»è¨˜éŒ²è¿½åŠ   
from datetime import datetime
# ç›®çš„: å±¥æ­´ç®¡ç†ã¨ãƒ•ã‚¡ã‚¤ãƒ«åä¸€æ„åŒ–

# Stage 4: çµ±è¨ˆåˆ†æè¿½åŠ 
show_statistics()
# ç›®çš„: ãƒ‡ãƒ¼ã‚¿ã®å‚¾å‘åˆ†æ
```

**å­¦ç¿’ä¾¡å€¤**:
- **æ®µéšçš„é–‹ç™º**: å°ã•ãå§‹ã‚ã¦ç¢ºå®Ÿã«æ‹¡å¼µ
- **ç›®çš„åˆ¥è¨­è¨ˆ**: ç”¨é€”ã«å¿œã˜ãŸé©åˆ‡ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªé¸æŠ
- **ä¿å®ˆæ€§å‘ä¸Š**: æ©Ÿèƒ½ã”ã¨ã«åˆ†é›¢ã•ã‚ŒãŸæ˜ç¢ºãªè¨­è¨ˆ

#### 4ï¸âƒ£ **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å®Ÿè·µ**

##### ğŸš¨ **2æ®µéšã®ã‚¨ãƒ©ãƒ¼å‡¦ç†**

```python
try:
    response = requests.get(url, headers=headers, timeout=10)
    response.raise_for_status()  # HTTPã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
    
    soup = BeautifulSoup(response.text, "html.parser")
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†...
    
except requests.exceptions.RequestException as e:
    print(f"ğŸš¨ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚„URLã‚’ç¢ºèªã—ã¦ãã ã•ã„")
except Exception as e:
    print(f"â“ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
```

**å­¦ç¿’å†…å®¹**:
- **å…·ä½“çš„ã‚¨ãƒ©ãƒ¼å‡¦ç†**: RequestExceptionã«ã‚ˆã‚‹é€šä¿¡ã‚¨ãƒ©ãƒ¼ã®ç‰¹å®š
- **ä¸€èˆ¬çš„ã‚¨ãƒ©ãƒ¼å‡¦ç†**: äºˆæœŸã—ãªã„å•é¡Œã¸ã®å¯¾å¿œ
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼**: åˆ†ã‹ã‚Šã‚„ã™ã„ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨å¯¾å‡¦æ³•æç¤º

##### ğŸ”§ **é˜²å¾¡çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°**

```python
if link and link.get_text(strip=True):  # ç©ºãƒã‚§ãƒƒã‚¯
    headlines.append(link)

href = link.get('href', '')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š

if href.startswith('http'):  # URLå½¢å¼åˆ¤å®š
    full_url = href
else:
    full_url = f"https://news.ycombinator.com/{href}"
```

**å­¦ç¿’æˆæœ**:
- **NULLå®‰å…¨**: å­˜åœ¨ã—ãªã„è¦ç´ ã¸ã®å¯¾å‡¦
- **ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼**: æœŸå¾…ã™ã‚‹å½¢å¼ã‹ã®äº‹å‰ãƒã‚§ãƒƒã‚¯
- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†**: ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã¸ã®ä»£æ›¿å‡¦ç†

#### 5ï¸âƒ£ **ãƒ©ã‚¤ãƒ–ãƒ©ãƒªé¸æŠã®æŸ”è»Ÿæ€§**

##### ğŸ¯ **ç›®çš„åˆ¥ãƒ©ã‚¤ãƒ–ãƒ©ãƒªé¸æŠ**

**ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼ˆmain2.pyï¼‰ã®é¸æŠ**:
```python
import requests
from bs4 import BeautifulSoup
# csv, datetime ã‚’å‰Šé™¤
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- **é«˜é€Ÿèµ·å‹•**: ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚é–“ã®çŸ­ç¸®
- **è»½é‡å®Ÿè¡Œ**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å‰Šæ¸›
- **ç†è§£ã—ã‚„ã™ã•**: è¤‡é›‘ã•ã®æ’é™¤
- **ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—å‘ã‘**: æ©Ÿèƒ½æ¤œè¨¼ã®è¿…é€ŸåŒ–

**æ‹¡å¼µç‰ˆï¼ˆmain.pyï¼‰ã®é¸æŠ**:
```python
import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime
# å…¨æ©Ÿèƒ½æ­è¼‰
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- **æ°¸ç¶šåŒ–**: ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã¨å¾Œå‡¦ç†
- **åˆ†ææ©Ÿèƒ½**: çµ±è¨ˆæƒ…å ±ã®æä¾›
- **å±¥æ­´ç®¡ç†**: ã„ã¤å–å¾—ã—ãŸã‹ã®è¨˜éŒ²
- **æœ¬æ ¼é‹ç”¨**: å®Ÿç”¨çš„ãªæ©Ÿèƒ½ã®ç¶²ç¾…

##### ğŸ’¡ **ã€Œå¿…è¦ãªåˆ†ã ã‘ä½¿ã†ã€ã®ç¾å­¦**

```python
# è»Šè¼ªã®å†ç™ºæ˜ã‚’ã—ãªã„
requests â†’ HTTPé€šä¿¡ã®æ¨™æº–å®Ÿè£…ã‚’åˆ©ç”¨
BeautifulSoup â†’ HTMLè§£æã®ä¿¡é ¼æ€§ã‚ã‚‹å®Ÿè£…ã‚’åˆ©ç”¨
csv â†’ CSVå½¢å¼ã®æ­£ç¢ºãªå®Ÿè£…ã‚’åˆ©ç”¨
datetime â†’ æ™‚åˆ»å‡¦ç†ã®å›½éš›æ¨™æº–å®Ÿè£…ã‚’åˆ©ç”¨

# ã—ã‹ã—ã€ä¸è¦ãªã‚‚ã®ã¯ä½¿ã‚ãªã„
ç”¨é€”ã«å¿œã˜ã¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’é¸æŠçš„ã«åˆ©ç”¨
```

### ğŸš€ ä»Šå¾Œã®æ”¹å–„ãƒ»ç™ºå±•ã‚¢ã‚¤ãƒ‡ã‚¢

#### ğŸ“ˆ **æ©Ÿèƒ½æ‹¡å¼µæ¡ˆ**

##### 1ï¸âƒ£ **è¤‡æ•°ã‚µã‚¤ãƒˆå¯¾å¿œ**
```python
sites = [
    {"name": "Hacker News", "url": "https://news.ycombinator.com/", "method": scrape_hackernews},
    {"name": "Product Hunt", "url": "https://www.producthunt.com/", "method": scrape_producthunt},
    {"name": "Reddit", "url": "https://www.reddit.com/r/programming/", "method": scrape_reddit},
]

def multi_site_scraper():
    all_news = []
    for site in sites:
        news = site['method'](site['url'])
        all_news.extend(news)
    return all_news
```

##### 2ï¸âƒ£ **ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½**
```python
def filter_news_by_keywords(news_data, keywords):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ããƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    filtered = []
    for item in news_data:
        for keyword in keywords:
            if keyword.lower() in item['title'].lower():
                filtered.append(item)
                break
    return filtered

# ä½¿ç”¨ä¾‹
tech_news = filter_news_by_keywords(news_data, ['python', 'javascript', 'AI', 'machine learning'])
```

##### 3ï¸âƒ£ **å®šæœŸå®Ÿè¡Œæ©Ÿèƒ½**
```python
import schedule
import time

def scheduled_scraping():
    """å®šæœŸçš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã¨ãƒ‡ãƒ¼ã‚¿è“„ç©"""
    print(f"ğŸ“… å®šæœŸå®Ÿè¡Œ: {datetime.now()}")
    news_data = enhanced_hackernews_scraper()
    
    # éå»ãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒ
    compare_with_previous_data(news_data)

# æ¯æ™‚å®Ÿè¡Œã®è¨­å®š
schedule.every().hour.do(scheduled_scraping)

while True:
    schedule.run_pending()
    time.sleep(60)
```

##### 4ï¸âƒ£ **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜**
```python
import sqlite3

def save_to_database(news_data):
    """SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜"""
    conn = sqlite3.connect('news_database.db')
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS news (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            url TEXT NOT NULL,
            type TEXT,
            length INTEGER,
            timestamp TEXT,
            UNIQUE(url)
        )
    ''')
    
    for item in news_data:
        cursor.execute('''
            INSERT OR IGNORE INTO news (title, url, type, length, timestamp)
            VALUES (?, ?, ?, ?, ?)
        ''', (item['title'], item['url'], item['type'], item['length'], item['timestamp']))
    
    conn.commit()
    conn.close()
```

##### 5ï¸âƒ£ **Web UIå¯¾å¿œ**
```python
from flask import Flask, render_template, jsonify

app = Flask(__name__)

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/news')
def get_news():
    news_data = enhanced_hackernews_scraper()
    return jsonify(news_data)

@app.route('/api/news/search/<keyword>')
def search_news(keyword):
    news_data = enhanced_hackernews_scraper()
    filtered = filter_news_by_keywords(news_data, [keyword])
    return jsonify(filtered)

if __name__ == '__main__':
    app.run(debug=True)
```

#### ğŸ¯ **æŠ€è¡“çš„æ”¹å–„æ¡ˆ**

##### 1ï¸âƒ£ **ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–**
```python
import concurrent.futures
import threading

def parallel_multi_site_scraping():
    """è¤‡æ•°ã‚µã‚¤ãƒˆã®ä¸¦åˆ—å–å¾—"""
    sites = [
        "https://news.ycombinator.com/",
        "https://www.reddit.com/r/programming/",
        "https://lobste.rs/",
    ]
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        futures = {executor.submit(fetch_hackernews_titles, site): site for site in sites}
        
        results = {}
        for future in concurrent.futures.as_completed(futures):
            site = futures[future]
            try:
                results[site] = future.result()
            except Exception as e:
                print(f"âŒ {site} ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
```

##### 2ï¸âƒ£ **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½**
```python
import pickle
from datetime import datetime, timedelta

def cached_scraping(cache_duration_minutes=30):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    cache_file = 'news_cache.pkl'
    
    try:
        with open(cache_file, 'rb') as f:
            cache_data = pickle.load(f)
            
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æ€§ç¢ºèª
        if datetime.now() - cache_data['timestamp'] < timedelta(minutes=cache_duration_minutes):
            print("ğŸ“¦ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
            return cache_data['news_data']
    except FileNotFoundError:
        pass
    
    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    print("ğŸŒ æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    news_data = enhanced_hackernews_scraper()
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    cache_data = {
        'news_data': news_data,
        'timestamp': datetime.now()
    }
    
    with open(cache_file, 'wb') as f:
        pickle.dump(cache_data, f)
    
    return news_data
```

##### 3ï¸âƒ£ **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ**
```python
import json

def load_config():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
    with open('config.json', 'r') as f:
        config = json.load(f)
    return config

# config.json
{
    "sites": {
        "hackernews": {
            "url": "https://news.ycombinator.com/",
            "enabled": true,
            "max_items": 20
        },
        "reddit": {
            "url": "https://www.reddit.com/r/programming/",
            "enabled": false,
            "max_items": 15
        }
    },
    "output": {
        "format": "csv",
        "include_timestamp": true,
        "include_statistics": true
    },
    "scraping": {
        "timeout": 10,
        "retry_count": 3,
        "user_agent": "Mozilla/5.0 ..."
    }
}
```

#### ğŸ›¡ï¸ **å“è³ªå‘ä¸Šæ¡ˆ**

##### 1ï¸âƒ£ **ãƒ†ã‚¹ãƒˆæ©Ÿèƒ½**
```python
import unittest
from unittest.mock import patch, Mock

class TestNewsScraper(unittest.TestCase):
    
    @patch('requests.get')
    def test_successful_scraping(self, mock_get):
        """æ­£å¸¸ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ"""
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¨­å®š
        mock_response = Mock()
        mock_response.text = '<td class="title"><a href="test.html">Test Title</a></td>'
        mock_response.raise_for_status.return_value = None
        mock_get.return_value = mock_response
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        result = fetch_hackernews_titles()
        
        # çµæœæ¤œè¨¼
        self.assertGreater(len(result), 0)
        self.assertEqual(result[0]['title'], 'Test Title')
    
    @patch('requests.get')
    def test_network_error_handling(self, mock_get):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
        mock_get.side_effect = requests.exceptions.RequestException("Network Error")
        
        with self.assertRaises(requests.exceptions.RequestException):
            fetch_hackernews_titles()

if __name__ == '__main__':
    unittest.main()
```

##### 2ï¸âƒ£ **ãƒ­ã‚°æ©Ÿèƒ½**
```python
import logging

def setup_logging():
    """ãƒ­ã‚°è¨­å®š"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('news_scraper.log'),
            logging.StreamHandler()
        ]
    )

def enhanced_scraper_with_logging():
    """ãƒ­ã‚°æ©Ÿèƒ½ä»˜ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        news_data = enhanced_hackernews_scraper()
        logger.info(f"âœ… {len(news_data)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—æˆåŠŸ")
        return news_data
    except Exception as e:
        logger.error(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {e}")
        raise
```

### ğŸ’¡ **é‡è¦ãªå­¦ç¿’æˆæœ**

#### ğŸ¯ **æŠ€è¡“çš„ç†è§£ã®æ·±åŒ–**

##### ğŸ“š **ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®çœŸä¾¡**
- **requests**: HTTPé€šä¿¡ã®è¤‡é›‘ã•ã‚’å®Œå…¨ã«éš è”½
- **BeautifulSoup**: HTMLè§£æã®å®‰å…¨æ€§ã¨æŸ”è»Ÿæ€§ã‚’æä¾›
- **csv**: ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã®æ¨™æº–çš„ãªå®Ÿè£…
- **datetime**: æ™‚åˆ»å‡¦ç†ã®å›½éš›æ¨™æº–å¯¾å¿œ

**å®Ÿæ„Ÿ**: ã€Œ**ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã•ã¾ã•ã¾**ã€- 4ã¤ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå®Œç’§ã«é€£æºã—ã¦é«˜æ©Ÿèƒ½ã‚¢ãƒ—ãƒªã‚’å®Ÿç¾

##### ğŸ”§ **å•é¡Œè§£æ±ºãƒ—ãƒ­ã‚»ã‚¹ã®ä½“ç³»åŒ–**
1. **ä»®èª¬å®Ÿè£…**: æƒ³å®šã•ã‚Œã‚‹æ–¹æ³•ã§åˆæœŸå®Ÿè£…
2. **å•é¡Œç™ºç”Ÿ**: æœŸå¾…ã™ã‚‹çµæœãŒå¾—ã‚‰ã‚Œãªã„
3. **ãƒ‡ãƒãƒƒã‚°èª¿æŸ»**: å®Ÿéš›ã®çŠ¶æ³ã‚’è©³ç´°ã«èª¿æŸ»
4. **åŸå› ç‰¹å®š**: ä»®å®šã¨ç¾å®Ÿã®å·®ç•°ã‚’ç™ºè¦‹
5. **ä¿®æ­£å®Ÿè£…**: æ­£ç¢ºãªæƒ…å ±ã«åŸºã¥ãå†å®Ÿè£…
6. **æˆåŠŸç¢ºèª**: æœŸå¾…ã™ã‚‹çµæœã®ç²å¾—

**ä¾¡å€¤**: ä»Šå¾Œã‚ã‚‰ã‚†ã‚‹é–‹ç™ºã§æ´»ç”¨ã§ãã‚‹æ™®éçš„ãªãƒ—ãƒ­ã‚»ã‚¹

##### ğŸ¨ **è¨­è¨ˆæ€æƒ³ã®å­¦ç¿’**
- **æ®µéšçš„é–‹ç™º**: å°ã•ãå§‹ã‚ã¦ç¢ºå®Ÿã«æ‹¡å¼µ
- **ç›®çš„åˆ¥å®Ÿè£…**: ç”¨é€”ã«å¿œã˜ãŸé©åˆ‡ãªæ©Ÿèƒ½é¸æŠ
- **ä¿å®ˆæ€§é‡è¦–**: ç†è§£ã—ã‚„ã™ãå¤‰æ›´ã—ã‚„ã™ã„è¨­è¨ˆ
- **ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£**: åˆ†ã‹ã‚Šã‚„ã™ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨é©åˆ‡ãªå‡¦ç†

#### ğŸš€ **å®Ÿç”¨çš„ä¾¡å€¤**

##### âœ… **å³åº§ã«æ´»ç”¨å¯èƒ½**
- **æƒ…å ±åé›†ãƒ„ãƒ¼ãƒ«**: æ—¥å¸¸çš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã«ä½¿ç”¨å¯èƒ½
- **å­¦ç¿’ç´ æ**: Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®æ•™æã¨ã—ã¦æ´»ç”¨
- **æ‹¡å¼µåŸºç›¤**: ã‚ˆã‚Šé«˜åº¦ãªãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ã®åŸºç¤

##### ğŸ“ˆ **ç™ºå±•å¯èƒ½æ€§**
- **ãƒ“ã‚¸ãƒã‚¹å¿œç”¨**: ç«¶åˆåˆ†æã€å¸‚å ´èª¿æŸ»ã€ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
- **ç ”ç©¶ç”¨é€”**: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã€è‡ªç„¶è¨€èªå‡¦ç†ã®ç´ æ
- **è‡ªå‹•åŒ–åŸºç›¤**: å®šæœŸå®Ÿè¡Œã€ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

## ğŸ‰ ç·è©•

Day 68ã®Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€**å®Ÿç”¨çš„ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™º**ã¨**åŸºç¤æŠ€è¡“ã®æ·±ã„ç†è§£**ã‚’åŒæ™‚ã«é”æˆã§ããŸéå¸¸ã«ä¾¡å€¤ã®é«˜ã„å­¦ç¿’ä½“é¨“ã§ã—ãŸã€‚

### âœ… **ç‰¹ã«ä¾¡å€¤ãŒã‚ã£ãŸå­¦ç¿’å†…å®¹**

1. **ãƒ©ã‚¤ãƒ–ãƒ©ãƒªé€£æºã®ç¾ã—ã•**: 4ã¤ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå®Œç’§ã«é€£æºã™ã‚‹è¨­è¨ˆã®ç†è§£
2. **å•é¡Œè§£æ±ºãƒ—ãƒ­ã‚»ã‚¹**: ãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè·µçš„ä½“é¨“
3. **æ®µéšçš„é–‹ç™º**: æ©Ÿèƒ½ã‚’æ®µéšçš„ã«è¿½åŠ ã™ã‚‹ç¾å®Ÿçš„ãªé–‹ç™ºæ‰‹æ³•
4. **å®Ÿç”¨æ€§ã¨å­¦ç¿’æ€§**: å³åº§ã«ä½¿ãˆã‚‹ãƒ„ãƒ¼ãƒ«ã§ã‚ã‚ŠãªãŒã‚‰æ•™è‚²çš„ä¾¡å€¤ã‚‚é«˜ã„

### ğŸ¯ **ä»Šå¾Œã¸ã®å±•é–‹**

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ç¿’å¾—ã—ãŸæŠ€è¡“ã¯ã€**ã‚ã‚‰ã‚†ã‚‹Webé–¢é€£é–‹ç™º**ã«ãŠã„ã¦åŸºç›¤æŠ€è¡“ã¨ã—ã¦æ´»ç”¨ã§ãã¾ã™ã€‚ç‰¹ã«**ãƒ‡ãƒ¼ã‚¿åé›†**ã€**APIé€£æº**ã€**æƒ…å ±åˆ†æ**ã¨ã„ã£ãŸåˆ†é‡ã«ãŠã„ã¦ã€ä»Šå›ã®çµŒé¨“ãŒç›´æ¥çš„ã«æ´»ã‹ã•ã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚

**Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®åŸºç¤ã‹ã‚‰å®Ÿç”¨ã¾ã§**ã‚’ç¶²ç¾…çš„ã«å­¦ç¿’ã§ããŸã€
