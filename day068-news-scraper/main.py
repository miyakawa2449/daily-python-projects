import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime

def enhanced_hackernews_scraper():
    """æ‹¡å¼µç‰ˆ Hacker News ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    print("ğŸŒ Hacker News æ‹¡å¼µç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼é–‹å§‹")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    response = requests.get("https://news.ycombinator.com/", headers=headers, timeout=10)
    response.raise_for_status()
    
    soup = BeautifulSoup(response.text, "html.parser")
    title_elements = soup.find_all(class_="title")
    
    news_data = []
    
    for title_elem in title_elements:
        link = title_elem.find("a")
        if link and link.get_text(strip=True):
            title = link.get_text(strip=True)
            href = link.get('href', '')
            
            # URLæ­£è¦åŒ–
            if href.startswith('http'):
                full_url = href
                url_type = "external"
            else:
                full_url = f"https://news.ycombinator.com/{href}"
                url_type = "internal"
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–
            news_item = {
                'title': title,
                'url': full_url,
                'type': url_type,
                'length': len(title),
                'timestamp': datetime.now().isoformat()
            }
            news_data.append(news_item)
    
    # çµæœè¡¨ç¤º
    print(f"ğŸ“° å–å¾—æˆåŠŸ: {len(news_data)}ä»¶")
    print("=" * 60)
    
    for i, item in enumerate(news_data[:15], 1):
        print(f"{i:2d}. {item['title']}")
        print(f"    ğŸ”— {item['url']}")
        print(f"    ğŸ“Š æ–‡å­—æ•°: {item['length']} | ã‚¿ã‚¤ãƒ—: {item['type']}")
        print()
    
    # CSVä¿å­˜
    save_to_csv(news_data)
    
    # çµ±è¨ˆæƒ…å ±
    show_statistics(news_data)
    
    return news_data

def save_to_csv(news_data):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜"""
    filename = f"hackernews_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['title', 'url', 'type', 'length', 'timestamp']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        
        writer.writeheader()
        writer.writerows(news_data)
    
    print(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")

def show_statistics(news_data):
    """çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
    if not news_data:
        return
    
    external_count = sum(1 for item in news_data if item['type'] == 'external')
    internal_count = len(news_data) - external_count
    avg_length = sum(item['length'] for item in news_data) / len(news_data)
    
    print("ğŸ“Š çµ±è¨ˆæƒ…å ±:")
    print(f"  â€¢ ç·ä»¶æ•°: {len(news_data)}ä»¶")
    print(f"  â€¢ å¤–éƒ¨ãƒªãƒ³ã‚¯: {external_count}ä»¶")
    print(f"  â€¢ å†…éƒ¨ãƒªãƒ³ã‚¯: {internal_count}ä»¶")
    print(f"  â€¢ å¹³å‡ã‚¿ã‚¤ãƒˆãƒ«é•·: {avg_length:.1f}æ–‡å­—")
    
    # æœ€é•·ãƒ»æœ€çŸ­ã‚¿ã‚¤ãƒˆãƒ«
    longest = max(news_data, key=lambda x: x['length'])
    shortest = min(news_data, key=lambda x: x['length'])
    
    print(f"  â€¢ æœ€é•·ã‚¿ã‚¤ãƒˆãƒ«: {longest['title'][:50]}... ({longest['length']}æ–‡å­—)")
    print(f"  â€¢ æœ€çŸ­ã‚¿ã‚¤ãƒˆãƒ«: {shortest['title']} ({shortest['length']}æ–‡å­—)")

if __name__ == "__main__":
    enhanced_hackernews_scraper()
